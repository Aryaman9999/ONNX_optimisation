{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryaman.sharma/transfer learning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define the model name (using a DistilBERT classifier fine-tuned on SST-2)\n",
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryaman.sharma/transfer learning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Prepare sample input text and tokenized input\n",
    "text = \"This movie was absolutely fantastic! I loved every moment of it.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_inference(model, inputs, num_runs=100):\n",
    "    # Warm-up runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(**inputs)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(**inputs)\n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch inference benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference Time: 0.025096 sec/run\n"
     ]
    }
   ],
   "source": [
    "pytorch_time = benchmark_pytorch_inference(model, inputs, num_runs=100)\n",
    "print(f\"PyTorch Inference Time: {pytorch_time:.6f} sec/run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model to ONNX ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "dummy_input_ids = dummy_inputs[\"input_ids\"]\n",
    "dummy_attention_mask = dummy_inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryaman.sharma/transfer learning/.venv/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "✅ Model exported to ONNX!\n"
     ]
    }
   ],
   "source": [
    "# Export the model to ONNX with dynamic axes for batch and sequence dimensions\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input_ids, dummy_attention_mask),\n",
    "    \"distilbert_sst2.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=11\n",
    ")\n",
    "print(\"✅ Model exported to ONNX!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ONNX Runtime session using CPU for consistency\n",
    "session = ort.InferenceSession(\"distilbert_sst2.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Convert the dummy inputs to NumPy arrays for ONNX\n",
    "onnx_inputs = {\n",
    "    \"input_ids\": dummy_input_ids.numpy(),\n",
    "    \"attention_mask\": dummy_attention_mask.numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_onnx_inference(session, inputs_np, num_runs=100):\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = session.run(None, inputs_np)\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = session.run(None, inputs_np)\n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Inference Time: 0.006174 sec/run\n"
     ]
    }
   ],
   "source": [
    "onnx_time = benchmark_onnx_inference(session, onnx_inputs, num_runs=100)\n",
    "print(f\"ONNX Inference Time: {onnx_time:.6f} sec/run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup (PyTorch / ONNX): 4.06x faster\n"
     ]
    }
   ],
   "source": [
    "# Compare the speeds\n",
    "speedup = pytorch_time / onnx_time\n",
    "print(f\"Speedup (PyTorch / ONNX): {speedup:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession(\"distilbert_sst2.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def run_onnx_inference(text):\n",
    "    # Tokenize the input text and get numpy arrays for inputs\n",
    "    inputs = tokenizer(text, return_tensors=\"np\")\n",
    "    \n",
    "    # ONNX Runtime expects the inputs as a dictionary mapping input names to numpy arrays.\n",
    "    # Our exported model takes \"input_ids\" and \"attention_mask\".\n",
    "    onnx_inputs = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"]\n",
    "    }\n",
    "    \n",
    "    # Run inference using ONNX Runtime\n",
    "    outputs = session.run(None, onnx_inputs)\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    # Optional: apply softmax to get probabilities\n",
    "    probs = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [[1.1796260e-04 9.9988204e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"This movie was absolutely fantastic! I loved every moment of it.\"\n",
    "probabilities = run_onnx_inference(text)\n",
    "print(\"Probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "predicted_index = np.argmax(probabilities, axis=1)[0]\n",
    "label = \"positive\" if predicted_index == 1 else \"negative\"\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
